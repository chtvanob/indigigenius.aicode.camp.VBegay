{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week One Final Project: NBA Salary Prediction and Exploration\n",
    "\n",
    "Where has week one gone! We have one more project for you to put a nice little bow on all of the hard work you've done so far. For this project, be persistent, be curious, and ask questions if you get stuck!\n",
    "\n",
    "## The Project\n",
    "\n",
    "You and your teammates will create one prediction model and *AT LEAST* three plots or charts. Everyone will present their model and their charts during the final session of the day.\n",
    "* Model predictions will be ranked according to their r-squared values and we will crown a winner!\n",
    "* Your plots should be driven by curiosity. Everyone will present at least one plot.\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "We've provided helper functions down below. If you need help remembering what they do, refer to the `airbnb_solution.ipynb` example in the `3 - Day Three` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Get Started: Reading and preparing the data\n",
    "\n",
    "You'll need to use a lot of existing libraries and packages to look at the data. The cell below imports what you need into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use these packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below limits how many columns to show at once. Some data can be quite large, and you don't want to overload your notebook. For the nba data we are looking at, we shouldn't run into this issue, but it's a good practice to run a cell like this when analyzing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read the data from the csv file. The dataset we will be using contains statistics and salary information for NBA players during the 2022-2023 season.\n",
    "\n",
    "[Link to dataset source and information](https://www.kaggle.com/datasets/jamiewelsh2/nba-player-salaries-2022-23-season)\n",
    "\n",
    "**You will be looking for features of the data that can model (or predict) any of the players' salaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data!\n",
    "nba_data = pd.read_csv(\"nba_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first five rows of the data using the `df.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 1: Splitting up text data that differs within the column\n",
    "\n",
    "Sometimes column data will be in text form, but you still want to use it in your analysis. The next five code cells help you to modify the data so that it can be used for this type of analysis.\n",
    "\n",
    "For example, some positions might have higher salaries than others, but you currently can't look at individual positions because they are all in one column.\n",
    "\n",
    "Since you can't examine text data, the code below will to split each position out into it's own column, and indicate with a `1` or `0` whether the player played that position. While there may seem like better ways to view this data from a human perspective, this is considered *'best practice'* in data science.\n",
    "\n",
    "The code below splits out the selected column (`Position`) into its own `dataframe`, making each value a new column. It then stores the 'dummy' dataframe in a new variable called `position_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for splitting out text data that differs by column, but is representing a category\n",
    "# like genre, or artist, but not something track name\n",
    "position_dummies = pd.get_dummies(nba_data[\"Position\"])\n",
    "position_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process creates new columns. The cell below stores all these new column header names in a variable called `team_dummies_columns` so that you can easily look it up later if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dummies_columns = position_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can take a look at those column names using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "position_dummies_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a dataframe that you can use, combine your two separate dataframes (`nba_data` and `position_dummies`) using the `pandas.concat()` method, which will concatenate `position_dummies` onto the end of `nba_data`.\n",
    "\n",
    "The cell below does this work, and then stores this new dataframe in a variable called `merged_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genre_dummies is it's own dataframe, I want to contatenate it onto my nba_data\n",
    "merged_data = pd.concat([nba_data, position_dummies], axis=1)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since you're going to be doing the rest of the analysis using the `nba_data` variable, put this `merged_data` information into that variable by reassigning `nba_data`.\n",
    "\n",
    "***Note: you may want to repeat this process for 'Team' if that interests you***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nba_data = merged_data\n",
    "nba_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function 2: Splitting up data in dictionary form\n",
    "\n",
    "Sometimes data will come in a dictionary form, which looks like:\n",
    "\n",
    "```\n",
    "{\n",
    "    'rock': 'False',\n",
    "    'rap': 'True',\n",
    "    'country': 'False,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "This data isn't in a great form to do numerical analysis with, so the code cell below can be useful to create individual columns based on the keys in the dictionary.\n",
    "\n",
    "There is no data that is in dictionary form in the NBA dataset we are looking at, so it doesn't need to be run. This notebook is set up so that you can use different data in the future, so keep this helper function around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T USE THIS IF YOU DONT HAVE ANY DATA IN A DICTIONARY FORM\n",
    "\n",
    "# Helper Function: Feature Engineering\n",
    "# Use this to turn dictionary columns into useful features\n",
    "# We use the genre column as an example\n",
    "\n",
    "column = \"column\"  # FEEL FREE TO CHANGE THIS\n",
    "number_to_keep = 100\n",
    "\n",
    "def process_col_name(col_name):\n",
    "    col_name_list = ast.literal_eval(col_name)\n",
    "    if not isinstance(col_name_list, list):\n",
    "        return []\n",
    "    return [dic['name'] for dic in col_name_list if isinstance(dic, dict) and 'name' in dic]\n",
    "\n",
    "nba_data[f'{column}_list'] = nba_data[column].apply(process_col_name)\n",
    "\n",
    "# Compute the frequency of each col_name member\n",
    "freq = pd.Series([name for sublist in nba_data[f'{column}_list'].tolist() for name in sublist]).value_counts()\n",
    "\n",
    "# Keep the top 100 most frequent col_name members\n",
    "top_col_name = freq[:number_to_keep].index.tolist()\n",
    "\n",
    "# Filter the lists in the column to only include top col_name members\n",
    "nba_data[f'{column}_list'] = nba_data[f'{column}_list'].apply(lambda x: [i for i in x if i in top_col_name])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "binary_matrix = pd.DataFrame(mlb.fit_transform(nba_data[f'{column}_list']), columns=mlb.classes_)\n",
    "\n",
    "# Clean the column names: keep only alphanumeric characters and underscores\n",
    "binary_matrix.columns = binary_matrix.columns.str.replace('[^0-9a-zA-Z_]', '', regex=True)\n",
    "\n",
    "# Now, concatenate the binary matrix with the original DataFrame\n",
    "new_feature_names = binary_matrix.columns\n",
    "nba_data = pd.concat([nba_data, binary_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Data: Plotting\n",
    "\n",
    "The two code cells below help you to look at data in different ways using the **bar** plot and **scatter** plot functionality of dataframes.\n",
    "\n",
    "***To complete the challenge, you will need to have three (3) different plots that told you something about the data.***\n",
    "\n",
    "Play around with different columns to look at and compare. You will need to create at least one new code cell to add your third plot, which can be either a bar plot or a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function: Two Bar Chart Plots\n",
    "groupby_variable = \"col_1\"\n",
    "y_value = \"col_2\"\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 6))\n",
    "nba_data.groupby(groupby_variable)[y_value].mean().plot(kind=\"bar\", ax=axs[0], title=f\"Average {y_value}\")\n",
    "nba_data.groupby(groupby_variable)[y_value].count().plot(kind=\"bar\", ax=axs[1], title=f\"Count for each Bucket\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function: Scatter Plot\n",
    "\n",
    "x_value = \"col_1\"\n",
    "y_value = \"col_2\"\n",
    "\n",
    "nba_data.plot(x=x_value, y=y_value, kind=\"scatter\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: Selecting features to explain something\n",
    "\n",
    "The code cell below will support you in training a model that predicts the salary of any player.\n",
    "\n",
    "This is a *'regression'* model, meaning that it tries to predict a value, as opposed to *'classification'* which identifies a singular class.\n",
    "\n",
    "Basically, you are looking for what combination of the columns, which we call features, can predict how much a player in this dataset was paid. The model training part then works out how much of a weight to put on those features.\n",
    "\n",
    "The cell below will give a list of all of the options that you can use for features (just don't use 'salary', that's what you're trying to predict).\n",
    "\n",
    "Select some of these columns to include in the `features` variable list (it can be longer than 3).\n",
    "\n",
    "***The output of this cell will have a lot of information***  \n",
    "What you are looking for is the `R**2` value under the `Validation Data Statistics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nba_data.columns`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function: Model Training\n",
    "# DO NOT USE YOUR TARGET COLUMN IN THE FEATURES\n",
    "features = [\"col_1\", \"col_2\", \"col_3\"]\n",
    "\n",
    "target = \"Salary\"  # LEAVE THIS ALONE\n",
    "model_type = \"linear regression\"  # Options: \"random forest\" or \"linear regression\"\n",
    "features_to_show = 15\n",
    "\n",
    "\n",
    "if model_type == \"random forest\":\n",
    "    model = RandomForestRegressor()\n",
    "elif model_type == \"linear regression\":\n",
    "    model = LinearRegression()\n",
    "\n",
    "shuffled_data = nba_data.sample(len(nba_data))  # Shuffle our data\n",
    "train_data = shuffled_data[:int(len(shuffled_data)*0.8)]\n",
    "validation_data = shuffled_data[int(len(shuffled_data)*0.8):]\n",
    "\n",
    "model.fit(train_data[features], train_data[target])\n",
    "\n",
    "train_data[f\"predicted_{target}\"] = model.predict(train_data[features])\n",
    "validation_data[f\"predicted_{target}\"] = model.predict(validation_data[features])\n",
    "\n",
    "# How do we measure our success?\n",
    "print(\"Training Data Statistics\")\n",
    "print(\"mean_absolute_error: \", mean_absolute_error(train_data[target], train_data[f\"predicted_{target}\"]))\n",
    "print(\"mean_squared_error\", mean_squared_error(train_data[target], train_data[f\"predicted_{target}\"]))\n",
    "print(\"R**2\", r2_score(train_data[target], train_data[f\"predicted_{target}\"]))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Validation Data Statistics\")\n",
    "print(\"mean_absolute_error: \", mean_absolute_error(validation_data[target], validation_data[f\"predicted_{target}\"]))\n",
    "print(\"mean_squared_error\", mean_squared_error(validation_data[target], validation_data[f\"predicted_{target}\"]))\n",
    "print(\"R**2\", r2_score(validation_data[target], validation_data[f\"predicted_{target}\"]))\n",
    "\n",
    "if model_type == \"random forest\":\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[-features_to_show:]  # sort top features\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Bar plot\n",
    "    ax.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title(f'Top {features_to_show} Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "if model_type == \"linear regression\":\n",
    "    coefficients = model.coef_\n",
    "    indices = np.argsort(np.abs(coefficients))[-features_to_show:]  # sort top features by magnitude\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Bar plot\n",
    "    ax.barh(range(len(indices)), coefficients[indices], color='g', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top {features_to_show} Feature Coefficients in Linear Regression')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
